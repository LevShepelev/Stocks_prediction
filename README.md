# ğŸ“ˆ Stockâ€¯Prediction MLOps Project

This repository contains an **endâ€‘toâ€‘end educational pipeline** for timeâ€‘series forecasting of Russian stocks using deep learning (LSTM/GRU) with a modern MLOps stack:

* **PyTorchâ€¯+â€¯PyTorchâ€¯Lightning** for model authoring and training
* **Hydra & OmegaConf** for hierarchical configuration management
* **ONNX + TensorRT** for portable, highâ€‘performance inference
* **NVIDIA Triton Inference Server** to serve the model in productionâ€like conditions
* **Poetry** for dependency management and packaging
* **preâ€‘commit** (BlackÂ +Â isortÂ +Â flake8Â +Â mypy) for code quality
* **pytest** for unit tests

---

## ğŸ—‚ï¸ Repository Structure

```text
.
â”œâ”€â”€ data/                 # Raw & processed datasets (â›”â€¯Gitâ€‘ignored)
â”œâ”€â”€ exports/
â”‚   â””â”€â”€ onnx/             # Exported ONNX models
â”œâ”€â”€ model_repo/           # Triton model repository (â›”â€¯Gitâ€‘ignored)
â”œâ”€â”€ notebooks/            # Exploration & prototyping notebooks
â”œâ”€â”€ stocks_prediction/
â”‚   â”œâ”€â”€ conf/             # Hydra configs (train_config.yaml, â€¦)
â”‚   â”œâ”€â”€ datamodules/      # LightningDataModule implementations
â”‚   â”œâ”€â”€ models/           # LSTM/GRU model definitions
â”‚   â”œâ”€â”€ train_loop.py     # âš™ï¸Â Main training script (Hydraâ€‘aware)
â”‚   â”œâ”€â”€ predict_and_plot.py
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ convert_and_deploy_triton.py   # ONNXÂ â†’ TensorRTÂ â†’ Triton helper
â”œâ”€â”€ Dockerfile.triton              # (optional) Build a custom Triton image
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md                      # â† you are here
```

---

## ğŸ—ï¸Â Setup

> **Prerequisites**
>
> * PythonÂ Â³.11 with CUDAâ€‘enabled GPU & drivers (for training / TRT)
> * DockerÂ â‰¥Â 24 with NVIDIA ContainerÂ Runtime
> * (Optional) CUDAâ€¯/ cuDNN locally for JIT previews

```bash
# 1. Clone & install deps
git clone https://github.com/yourâ€‘org/Stocks_prediction.git
cd Stocks_prediction

# 2. Install poetry & project packages
curl -sSL https://install.python-poetry.org | python -
poetry install --with dev

# 3. Activate virtualâ€‘env
poetry shell

# 4. Set environment variables (API keys, tracking URLsâ€¦)
cp .env.example .env  # then edit values
```

---

## ğŸš‚Â Training

Hydra drives the pipeline; all hyperâ€‘parameters live in **`stocks_prediction/conf/`**.

```bash
# Train with the default config
poetry run python stocks_prediction/train_loop.py

# Override any parameter on the CLI (Hydra syntax)
poetry run python stocks_prediction/train_loop.py \
  model=stockgru \
  datamodule.batch_size=256 \
  trainer.max_epochs=50

# Log files, checkpoints & TensorBoard go to ./outputs/<DATE_TIME>/
```

### ğŸ”„Â Resume or Fineâ€‘Tune

```bash
poetry run python stocks_prediction/train_loop.py \
  +checkpoint_path=outputs/2025â€‘05â€‘31/15â€‘42â€‘10/checkpoints/epoch=04.ckpt
```

---

## ğŸ“¤Â Export to ONNX

Training script already exports the **best checkpoint** to ONNX (`exports/onnx/`).
To reâ€‘export any Lightning checkpoint:

```bash
poetry run python -m stocks_prediction.export_onnx \
  --ckpt_path outputs/â€¦/model.ckpt \
  --seq_len 60 --feature_dim 5
```

---

## âš™ï¸Â Convert to TensorRTÂ & Deploy to Triton

```bash
# 1ï¸âƒ£Â Convert ONNX â†’ TensorRT engine and generate Triton directory
poetry run python convert_and_deploy_triton.py \
  --onnx exports/onnx/stocklstm_GAZP_10m_2002â€‘01â€‘01_2025â€‘05â€‘19.onnx \
  --max-batch 32 \
  --seq-len 60 \
  --feature-dim 5 \
  --repo-dir ../model_repo/

# The script creates
#   model_repo/
#       stocklstm/
#           1/model.plan   (TensorRT engine)
#           config.pbtxt   (I/O schema)
```

---

## ğŸš€Â Run Triton Inference Server

```bash
# Pull NVIDIA Triton image (â‰ˆ3â€¯GB)
docker pull nvcr.io/nvidia/tritonserver:25.04-py3

# Launch server mapping HTTPÂ :8000, gRPCÂ :8001, metricsÂ :8002
#Â (replace $(pwd)/../model_repo with your absolute path)
docker run --rm --gpus=all --name triton_server \
  -p8011:8000 -p8012:8001 -p8013:8002 \
  -v "$(pwd)/../model_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 \
  tritonserver --model-repository=/models
```

> Visit **`http://localhost:8011/v2/health/ready`** â‡’ `OK` when ready.

---

## ğŸ”®Â Batch Inference & Plotting

For lightweight local experiments without Triton:

```bash
poetry run python stocks_prediction/predict_and_plot.py \
  --data_dir ../data/raw/ \
  --ckpt_path outputs/â€¦/best.ckpt
```

### ğŸ”ŒÂ Triton Client Example

```python
import numpy as np
import tritonclient.http as http

client = http.InferenceServerClient(url="localhost:8011")
X = np.load("sample_batch.npy").astype(np.float32)  # (B, T, F)
inputs  = http.InferInput("INPUT__0", X.shape, "FP32")
inputs.set_data_from_numpy(X)
result = client.infer("stocklstm", inputs=[inputs])
print(result.as_numpy("OUTPUT__0"))
```

---

## ğŸ› ï¸Â Development

| Task                   | Command                                 |
| ---------------------- | --------------------------------------- |
| **FormatÂ +â€¯Lint**      | `poetry run pre-commit run --all-files` |
| **Typeâ€‘check**         | `poetry run mypy stocks_prediction/`    |
| **Tests**              | `poetry run pytest -q`                  |
| **Build wheel**        | `poetry build`                          |
| **Publish (internal)** | `poetry publish --username __token__`   |

### Continuous Integration

Add in `.github/workflows/ci.yml` (sample provided) to automate lint â†’ test â†’ build.

---

## ğŸ“ŠÂ Monitoring & Logging

* **Weights & Biases** integration is toggled via `wandb.enabled=true` in Hydra.
* Training logs: `outputs/<run>/train.log` (rotating, JSON).
* Triton exposes Prometheus metrics at **`:8002/metrics`**; a Grafana dashboard `dockerâ€‘compose.monitoring.yml` is included.

---

## ğŸ§¹Â Cleaning Artifacts

```bash
# Remove Hydra outputs (confirm *first*)
find outputs -maxdepth 1 -mtime +7 -type d -exec rm -r {} +
# Purge old TensorBoard event files
rm -rf lightning_logs/
```

---

## ğŸ“„Â License

This project is licensed under the **MIT License** â€“ see [`LICENSE`](LICENSE) for details.

---

## ğŸ™‹â€â™‚ï¸Â FAQ

| Question           | Answer                                                                                                                    |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------- |
| Dataset source?    | Minuteâ€‘level OHLCV data for MOEX tickers, collected via [ISSÂ API](https://iss.moex.com) (script `data/download_moex.py`). |
| CPUâ€‘only training? | Possible; set `trainer.accelerator=cpu`, but expect Ã—20 slower.                                                           |
| Windows supported? | Training works under WSLÂ 2; TensorRT/Triton require Linux.                                                                |

---

> Feel free to open an issue or discussion if you hit a snag. Happy forecasting! \:rocket:
