# ðŸ“ˆ Stockâ€¯Prediction Project

This repository contains an **endâ€‘toâ€‘end educational pipeline** for timeâ€‘series
forecasting of Russian stocks using deep learning (LSTM/GRU) with a modern MLOps
stack:

- **PyTorchâ€¯+â€¯PyTorchâ€¯Lightning** for model authoring and training
- **Hydra & OmegaConf** for hierarchical configuration management
- **ONNX + TensorRT** for portable, highâ€‘performance inference
- **NVIDIA Triton Inference Server** to serve the model in productionâ€like
  conditions
- **Poetry** for dependency management and packaging
- **preâ€‘commit** (BlackÂ +Â isortÂ +Â flake8Â +Â mypy) for code quality

---

## ðŸ—‚ï¸ Repository Structure

```text
.
â”œâ”€â”€ data/                 # Raw & processed datasets (â›”â€¯Gitâ€‘ignored)
â”œâ”€â”€ exports/
â”‚   â””â”€â”€ onnx/             # Exported ONNX models
â”œâ”€â”€ model_repo/           # Triton model repository (â›”â€¯Gitâ€‘ignored)
â”œâ”€â”€ notebooks/            # Exploration & prototyping notebooks
â”œâ”€â”€ stocks_prediction/
â”‚   â”œâ”€â”€ conf/             # Hydra configs (train_config.yaml, â€¦)
â”‚   â”œâ”€â”€ datamodules/      # LightningDataModule implementations
â”‚   â”œâ”€â”€ models/           # LSTM/GRU model definitions
â”‚   â”œâ”€â”€ train_loop.py     # âš™ï¸Â Main training script (Hydraâ€‘aware)
â”‚   â”œâ”€â”€ predict_and_plot.py
â”‚   â””â”€â”€ â€¦
â”œâ”€â”€ convert_and_deploy_triton.py   # ONNXÂ â†’ TensorRTÂ â†’ Triton helper
â”œâ”€â”€ Dockerfile.triton              # (optional) Build a custom Triton image
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md                      # â† you are here
```

---

## ðŸ—ï¸Â Setup

> **Prerequisites**
>
> - PythonÂ Â³.11 with CUDAâ€‘enabled GPU & drivers (for training / TRT)
> - DockerÂ â‰¥Â 24 with NVIDIA ContainerÂ Runtime
> - (Optional) CUDAâ€¯/ cuDNN locally for JIT previews

```bash
# 1. Clone & install deps
git clone https://github.com/yourâ€‘org/Stocks_prediction.git
cd Stocks_prediction

# 2. Install poetry & project packages
curl -sSL https://install.python-poetry.org | python -
poetry install --with dev

# 3. Activate virtualâ€‘env
poetry shell

# 4. Set environment variables (API keys, tracking URLsâ€¦)
cp .env.example .env  # then edit values
```

---

## ðŸš‚Â Training

Hydra drives the pipeline; all hyperâ€‘parameters live in
**`stocks_prediction/conf/`**.

```bash
# Train with the default config
poetry run python stocks_prediction/train_loop.py

# Override any parameter on the CLI (Hydra syntax)
poetry run python stocks_prediction/train_loop.py \
  model=stockgru \
  datamodule.batch_size=256 \
  trainer.max_epochs=50

# Log files, checkpoints & TensorBoard go to ./outputs/<DATE_TIME>/
```

### ðŸ”„Â Resume or Fineâ€‘Tune

```bash
poetry run python stocks_prediction/train_loop.py \
  +checkpoint_path=outputs/2025â€‘05â€‘31/15â€‘42â€‘10/checkpoints/epoch=04.ckpt
```

---

## ðŸ“¤Â Export to ONNX

Training script already exports the **best checkpoint** to ONNX
(`exports/onnx/`).

---

## âš™ï¸Â Convert to TensorRTÂ & Deploy to Triton

```bash
# 1ï¸âƒ£Â Convert ONNX â†’ TensorRT engine and generate Triton directory
poetry run python convert_and_deploy_triton.py \
  --onnx exports/onnx/stocklstm_GAZP_10m_2002â€‘01â€‘01_2024â€‘05â€‘01.onnx \
  --repo-dir ../model_repo/

# The script creates
#   model_repo/
#       stocklstm/
#           1/model.plan   (TensorRT engine)
#           config.pbtxt   (I/O schema)
```

---

## ðŸš€Â Run Triton Inference Server

```bash
# Pull NVIDIA Triton image (â‰ˆ3â€¯GB)
docker pull nvcr.io/nvidia/tritonserver:25.04-py3

# Launch server mapping HTTPÂ :8000, gRPCÂ :8001, metricsÂ :8002
#Â (replace $(pwd)/../model_repo with your absolute path)
docker run --rm --gpus=all --name triton_server \
  -p8011:8000 -p8012:8001 -p8013:8002 \
  -v "$(pwd)/../model_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 \
  tritonserver --model-repository=/models
```

> Visit **`http://localhost:8011/v2/health/ready`** â‡’ `OK` when ready.

### ðŸš Shell script helper

A reusable helper to (re)start Triton locally is provided in
**`scripts/start_triton.sh`**:

```bash
#!/usr/bin/env bash
# scripts/start_triton.sh
set -euo pipefail

# Absolute path to repo root
ROOT_DIR="$(git rev-parse --show-toplevel)"

# Default ports can be overridden via env vars
HTTP_PORT="${HTTP_PORT:-8011}"
GRPC_PORT="${GRPC_PORT:-8012}"
METRICS_PORT="${METRICS_PORT:-8013}"

# Launch (remove --rm if you want persistent logs)
docker run --rm --gpus=all --name triton_server \
  -p"${HTTP_PORT}":8000 -p"${GRPC_PORT}":8001 -p"${METRICS_PORT}":8002 \
  -v "${ROOT_DIR}/model_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 \
  tritonserver --model-repository=/models "$@"
```

Make it executable and run:

```bash
chmod +x scripts/start_triton.sh
./scripts/start_triton.sh
```

---

## ðŸ”®Â Batch Inference & Plotting

For lightweight local experiments without Triton:

```bash
poetry run python predict_and_plot.py   --data_dir ../data/raw/
```

### ðŸ”ŒÂ Triton Client Example

```bash
poetry run python inference/triton/client_lstm.py --data_dir ../data/raw/
```

---

## ðŸ› ï¸Â Development

| Task                   | Command                                 |
| ---------------------- | --------------------------------------- |
| **FormatÂ +â€¯Lint**      | `poetry run pre-commit run --all-files` |
| **Build wheel**        | `poetry build`                          |
| **Publish (internal)** | `poetry publish --username __token__`   |

### Continuous Integration

Add in `.github/workflows/ci.yml` (sample provided) to automate lint â†’ test â†’
build.

---

## ðŸ“„Â License

This project is licensed under the **MIT License** â€“ see [`LICENSE`](LICENSE)
for details.

---

## ðŸ™‹â€â™‚ï¸Â FAQ

| Question           | Answer                                                                                                                       |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| Dataset source?    | 10 Minuteâ€‘level OHLCV data for MOEX tickers, collected via [ISSÂ API](https://iss.moex.com) (script `data/download_moex.py`). |
| CPUâ€‘only training? | Possible; set `trainer.accelerator=cpu`, but expect Ã—20 slower.                                                              |
| Windows supported? | Training works under WSLÂ 2; TensorRT/Triton require Linux.                                                                   |

---

> Feel free to open an issue or discussion if you hit a snag. Happy forecasting!
