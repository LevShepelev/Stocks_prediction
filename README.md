# üìà Stock‚ÄØPrediction Project

This repository contains an **end‚Äëto‚Äëend educational pipeline** for time‚Äëseries
forecasting of Russian stocks using deep learning (LSTM) with a modern MLOps
stack:

- **PyTorch‚ÄØ+‚ÄØPyTorch‚ÄØLightning** for model authoring and training
- **Hydra & OmegaConf** for hierarchical configuration management
- **ONNX + TensorRT** for portable, high‚Äëperformance inference
- **NVIDIA Triton Inference Server** to serve the model in production‚Äêlike
  conditions
- **Poetry** for dependency management and packaging
- **pre‚Äëcommit** (Black¬†+¬†isort¬†+¬†flake8¬†+¬†mypy) for code quality

---

## üóÇÔ∏è Repository Structure

```text
.
‚îú‚îÄ‚îÄ data/                 # Raw & processed datasets (‚õî‚ÄØGit‚Äëignored)
‚îú‚îÄ‚îÄ exports/
‚îÇ   ‚îî‚îÄ‚îÄ onnx/             # Exported ONNX models
‚îú‚îÄ‚îÄ model_repo/           # Triton model repository (‚õî‚ÄØGit‚Äëignored)
‚îú‚îÄ‚îÄ notebooks/            # Exploration & prototyping notebooks
‚îú‚îÄ‚îÄ stocks_prediction/
‚îÇ   ‚îú‚îÄ‚îÄ conf/             # Hydra configs (train_config.yaml, ‚Ä¶)
‚îÇ   ‚îú‚îÄ‚îÄ datamodules/      # LightningDataModule implementations
‚îÇ   ‚îú‚îÄ‚îÄ models/           # LSTM/GRU model definitions
‚îÇ   ‚îú‚îÄ‚îÄ train_loop.py     # ‚öôÔ∏è¬†Main training script (Hydra‚Äëaware)
‚îÇ   ‚îú‚îÄ‚îÄ predict_and_plot.py
‚îÇ   ‚îî‚îÄ‚îÄ ‚Ä¶
‚îú‚îÄ‚îÄ convert_and_deploy_triton.py   # ONNX¬†‚Üí TensorRT¬†‚Üí Triton helper
‚îú‚îÄ‚îÄ Dockerfile.triton              # (optional) Build a custom Triton image
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ README.md                      # ‚Üê you are here
```

---

## üèóÔ∏è¬†Setup

> **Prerequisites**
>
> - Python¬†¬≥.11 with CUDA‚Äëenabled GPU & drivers (for training / TRT)
> - Docker¬†‚â•¬†24 with NVIDIA Container¬†Runtime
> - (Optional) CUDA‚ÄØ/ cuDNN locally for JIT previews

```bash
# 1. Clone & install deps
git clone https://github.com/your‚Äëorg/Stocks_prediction.git
cd Stocks_prediction

# 2. Install poetry & project packages
curl -sSL https://install.python-poetry.org | python -
poetry install --with dev

# 3. Activate virtual‚Äëenv
poetry shell

# 4. Set environment variables (API keys, tracking URLs‚Ä¶)
cp .env.example .env  # then edit values
```
‚¨áÔ∏è¬†Download Data with DVC

This project uses DVC to manage and version the raw OHLCV datasets.

# Ensure DVC is installed in your environment
poetry run pip install dvc

# Configure your DVC remote (only first time):
# e.g., an S3 bucket, Azure Blob, Google Drive, etc.
# dvc remote add -d storage <remote-url>

# Pull the raw and processed data files
poetry run dvc pull -r storage data/

Note: Replace <remote-url> and storage with your DVC remote name and URL as specified by your project maintainers.


## üöÇ¬†Training

Hydra drives the pipeline; all hyper‚Äëparameters live in
**`stocks_prediction/conf/`**.

```bash
# Train with the default config
poetry run python stocks_prediction/train_loop.py
# Log files, checkpoints & TensorBoard go to ./outputs/<DATE_TIME>/
```

### üîÑ¬†Resume or Fine‚ÄëTune

```bash
poetry run python stocks_prediction/train_loop.py \
  +checkpoint_path=outputs/2025‚Äë05‚Äë31/15‚Äë42‚Äë10/checkpoints/epoch=04.ckpt
```

---

## üì§¬†Export to ONNX

Training script already exports the **best checkpoint** to ONNX
(`exports/onnx/`).

---

## ‚öôÔ∏è¬†Convert to TensorRT¬†& Deploy to Triton

```bash
# 1Ô∏è‚É£¬†Convert ONNX ‚Üí TensorRT engine and generate Triton directory
poetry run python convert_and_deploy_triton.py \
  --onnx exports/onnx/stocklstm_GAZP_10m_2002‚Äë01‚Äë01_2024‚Äë05‚Äë01.onnx \
  --repo-dir ../model_repo/

# The script creates
#   model_repo/
#       stocklstm/
#           1/model.plan   (TensorRT engine)
#           config.pbtxt   (I/O schema)
```

---

## üöÄ¬†Run Triton Inference Server

```bash
# Pull NVIDIA Triton image (‚âà3‚ÄØGB)
docker pull nvcr.io/nvidia/tritonserver:25.04-py3

# Launch server mapping HTTP¬†:8000, gRPC¬†:8001, metrics¬†:8002
#¬†(replace $(pwd)/../model_repo with your absolute path)
docker run --rm --gpus=all --name triton_server \
  -p8011:8000 -p8012:8001 -p8013:8002 \
  -v "$(pwd)/../model_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 \
  tritonserver --model-repository=/models
```

> Visit **`http://localhost:8011/v2/health/ready`** ‚áí `OK` when ready.

### üêö Shell script helper

A reusable helper to (re)start Triton locally is provided in
**`scripts/start_triton.sh`**:

```bash
#!/usr/bin/env bash
# scripts/start_triton.sh
set -euo pipefail

# Absolute path to repo root
ROOT_DIR="$(git rev-parse --show-toplevel)"

# Default ports can be overridden via env vars
HTTP_PORT="${HTTP_PORT:-8011}"
GRPC_PORT="${GRPC_PORT:-8012}"
METRICS_PORT="${METRICS_PORT:-8013}"

# Launch (remove --rm if you want persistent logs)
docker run --rm --gpus=all --name triton_server \
  -p"${HTTP_PORT}":8000 -p"${GRPC_PORT}":8001 -p"${METRICS_PORT}":8002 \
  -v "${ROOT_DIR}/model_repo:/models" \
  nvcr.io/nvidia/tritonserver:25.04-py3 \
  tritonserver --model-repository=/models "$@"
```

Make it executable and run:

```bash
chmod +x scripts/start_triton.sh
./scripts/start_triton.sh
```

---

## üîÆ¬†Batch Inference & Plotting

For lightweight local experiments without Triton:

```bash
poetry run python predict_and_plot.py   --data_dir ../data/raw/
```

### üîå¬†Triton Client Example

```bash
poetry run python inference/triton/client_lstm.py --data_dir ../data/raw/
```

---

## üõ†Ô∏è¬†Development

| Task                   | Command                                 |
| ---------------------- | --------------------------------------- |
| **Format¬†+‚ÄØLint**      | `poetry run pre-commit run --all-files` |
| **Build wheel**        | `poetry build`                          |
| **Publish (internal)** | `poetry publish --username __token__`   |

### Continuous Integration

Add in `.github/workflows/ci.yml` (sample provided) to automate lint ‚Üí test ‚Üí
build.

---

## üìÑ¬†License

This project is licensed under the **MIT License** ‚Äì see [`LICENSE`](LICENSE)
for details.

---

## üôã‚Äç‚ôÇÔ∏è¬†FAQ

| Question           | Answer                                                                                                                       |
| ------------------ | ---------------------------------------------------------------------------------------------------------------------------- |
| Dataset source?    | 10 Minute‚Äëlevel OHLCV data for MOEX tickers, collected via [ISS¬†API](https://iss.moex.com) (script `data/download_moex.py`). |
| CPU‚Äëonly training? | Possible; set `trainer.accelerator=cpu`, but expect √ó20 slower.                                                              |
| Windows supported? | Training works under WSL¬†2; TensorRT/Triton require Linux.                                                                   |

---

> Feel free to open an issue or discussion if you hit a snag. Happy forecasting!
